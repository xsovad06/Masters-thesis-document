\kap[chapter:practical-part]{Practical part}
In this chapter, we will comprehensively examine the various stages involved in training a new model for human pose estimation. The initial phase entails the preparation of a custom dataset, followed by the utilization of an existing models outlined in \in{Section}[section:existing-nns] on \at{page}[section:existing-nns]. Subsequently, a unified format for pose estimation is introduced to aggregate results from each model.

dataset. Throughout this thesis, several implementation challenges emerged, leading to certain technical limitations outlined in a dedicated \in{Section}[section:problems-limitations] at the end of this chapter.

\pkap[section:individual-models-detection]{Individual Models detection}
This section introduces an approach to the individual models detection implementation. It shows the key concepts and tools for creating complex detection scripts in {\bf Python} programming language.

For the implementation the main tool is the Python and its libraries. In the initial step of creating the detection scripts for individual model, the following libraries were used:

\startitemize[n]
    \item PoseNet:
        \startitemize[1]
            \item {\bf {\em Mediapipe}} - A library containing the PoseNet model and tools for drawing the detections on image.
        \stopitemize
    \item MoveNet:
        \startitemize[1]
            \item {\bf {\em Tensorflow}} - A library used for working with the structures representing the image data.
            \item {\bf {\em Tensorflow Hub}} - A repository of trained machine learning models.
        \stopitemize
    \item MMPose:
        \startitemize[1]
            \item {\bf {\em MMCV}} - A foundational library for computer vision research.
            \item {\bf {\em MMPOSE}} - An open-source toolbox for pose estimation based on {\bf PyTorch}.
            \item {\bf {\em MMDET}} - An open-source object detection toolbox based on {\bf PyTorch}.
        \stopitemize
\stopitemize

The main idea behind all detection scripts is to provide very similar interface with the same funcionality. Each detection model uses different resources, meaning that the detection part itself is different for every case and needs to be implemented separately. Similarly, the model initialization parts is different. However, the script skeleton is the same for every model. The scripts use the common custom functions which are gathered to the {\em utils} package. These are the methods for manipulation with the images to provide the same frame formats for every model. Then there are helper functions for processing the script arguments like handling the processed file paths, output file names, exporting the detections to JSON files etc. Additionally, the functions for drawing the keypoints and skeletons on detedted human instances for MoveNet model for which there is no library with this functionality unlike the other two models. See the script structure in the following code:

\sourcecode{Simplified detection script backbone. Containing the common code base used for each of the detection models. Accorging to a given program arguments, the scrit processes files of differrent type. Accepts the image and video files. If the given input is a directory, the script loops through the all video and image files and executes the detection on them. Finally, every detection is saved into a JSON file.}{}{src:detection-script-backbone}{}

\pkap[section:dataset]{Dataset}
In this section, the COCO WholeBody dataset (Xu et al. 2022) will be used as a corpus for the unified detection format evaluation. Additionally, the manipulations and details regarding the processing and use of the dataset will be investigated. Finally, the specified custom dataset with healthcare usecases will be described.

As for the unified detection format evaluation, the {\bf COCO WholeBody} dataset was used. Specifically the {\bf validation} subset. The total (\APE\) was equal to {\bf 4.37\%}. This metric was calculated only on the matching instances of the predicted annotations with respect to the ground truth annotations. This means that the differences in the predictions and the ground truth were not taken into an account. Specifically the missing or additional (detection) instances in the predictions. But as for the corresponding instances, the evaluation serves as a great indicator of how well the individual detection instances wer obtained. The criterion used for defining whether the instances from predictions correspond to the ground truth annotations is the bounding box ovelap. The treshold used for the overlap value, {\bf Intersection over Union} (\IoU\), is the {\bf 65\%}. To better understand the \IoU\ metric see the \in{Figure}[iou].

\obrazek{iou}{Calculation of the Intersection over Union}{figures/iou.png}{width=\makeupwidth}

There was also made an evaluation on the \APE\ metric just for the {\bf MMPose} model because only this model provides us with the bounding boxes in the detection. The total \APE\ was: {\bf 1.29\%} which provided us with the information about the model precision on individual keypoints of detected instance. This information will be used for setting up the weights for weighted average in the process of unified format creation in the following \in{section}[section:unified-format] on \at{page}[section:unified-format].

Because of the fact, that the human pose estimation is very complex task, it is required to use multiple metrics to expres the perfomance of the detection models. For this reason additionally, the mean squeare error is evaulated as well.

\pkap[section:unified-format]{Created Unified Format}
A~highly cost-effective method of obtaining a custom dataset with real-life footage data is to leverage existing \NN\-s to generate labels. This approach enables the training of a new model specifically tailored to the target detection task. To develop a novel model with the desired functionality, multiple models can be employed for label generation. However, for effective training, a unified format is required to aggregate the results of these individual models. This section precisely addresses the concept of a unified format introduced in this thesis, capitalizing on the strengths of existing models. As explained in the previous chapter, each model estimates a different number of keypoints, emphasizing different qualities. Refer to \in{Table}[tab:format-comparison] for a comprehensive understanding of these differences.

The rationale behind the unified format is to identify commonalities among individual formats and address their variations. Essentially, the common format is based on {\bf MoveNet}, which comprises {\bf 17} keypoints, excluding {\em hands} and {\em feet} estimation compared to the unified format. {\bf PoseNet} introduces an additional eyes keypoints (compared to unified format) that needs elimination, while {\bf MMPose} includes {\bf 107} unnecessary keypoints, particularly detailed facial keypoints and non-crucial hand keypoints (individual joints of each finger). The unified format optimally encompasses {\bf 27} keypoints, providing satisfactory detail for hands, feet, and face. Refer to \in{Figure}[unified-format-structure] for a visual representation of the structure. You can see that the format is very simple and straightforward. It offers basic pose representation with sufficient detail to both hands and feet.

Another crucial aspect is the accurate aggregation of individual model estimations, encompassing both the coordinates of keypoints and their visibility values. To address this, we introduce a weighted average, mitigating weaknesses in faster models such as MoveNet and PoseNet. Given that these models are designed for real-time estimation, and we utilize the "lightning" model version for PoseNet, accuracy is inherently limited. Detailed values for the weighted average are available in \in{Table}[tab:format-comparison]. The assignment of the highest weight to the {\bf MMPose} model is justified by its superior accuracy on our custom dataset. This approach ensures the uniformity of estimations made by individual models across the entire dataset, as prepared in the previous section.

The weighted average is not applied in scenarios involving the processing of keypoints on the hands or feet. This limitation arises from the fact that the {\bf MoveNet} model does not provide these keypoints, necessitating a basic average calculation of the two values. In other words, the weighted average is applied only when there is data from all three models.

\TABULKA[][tab:format-comparison]{Comparison of the individual models detection format}
    \setupTABLE[r][1][style=bold]
    \setupTABLE[c][each][offset=3dd]
    \setupTABLE[frame=off]
    \setupTABLE[r][1][topframe=on,bottomframe=on]
    \setupTABLE[c][each][leftframe=on]
    \setupTABLE[c][1][leftframe=off]
    \setupTABLE[c][2,3,4][align=middle]
    \bTR
        \bTD Model\eTD\bTD      Keypoints\eTD\bTD  Multi-person detection\eTD\bTD  Weight\eTD\eTR
    \bTR
        \bTD PoseNet\eTD\bTD    33\eTD\bTD  No\eTD\bTD  0.3\eTD\eTR
    \bTR
        \bTD MoveNet\eTD\bTD    17\eTD\bTD  No\eTD\bTD  0.2\eTD\eTR
    \bTR
        \bTD MMPose\eTD\bTD     133\eTD\bTD  Yes\eTD\bTD  0.5\eTD\eTR
    \bTR
        \bTD Unified Format\eTD\bTD     27\eTD\bTD  Yes\eTD\bTD  --\eTD\eTR


\obrazek{unified-format-structure}{Unified format structure with IDs to each keypoint}{figures/unified-detection-structure.png}{width=\makeupwidth}

\pkap[section:experiments]{Experiments and Results}

\pkap[section:problems-limitations]{Implementation Problems and Technical Limitations}
