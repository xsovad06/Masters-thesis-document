\kap[chapter:theoretical-foundation]{Theoretical Foundations}
This chapter provides an overview of the theoretical foundations of the proposed automated \NN\-s dataset generation approach for human skeleton detection. It explains the difficulties of the human pose estimation in the real-world enviroment. Additionally, it introduces the key concepts of \NN\-s, convolutional neural network (\CNN\) and region-based convolutional neural network (\RCNN\). Additionally, it explores existing \NN\-s for human pose estimation, including {\em PoseNet}, {\em MoveNet}, and {\em MMPose}. Finally, the detection performance evaluation metrics are described.

% ------- Section ------- %
\pkap[section:pose-estimation-challanges]{Challenges in Real-World Human Pose Estimation}
Human pose estimation faces numerous challenges in real-world applications, such as smart surveillance. Surveillance cameras are deployed in diverse locations, including shopping malls, stores, hallways, food courts, and parking lots. These locations present varying distances between individuals and cameras, occlusions, and crowded scenes. Three primary challenges were identified:

\startitemize[n]
    \item {\bf Wide Variety of Distances:} This refers to the varying scales of individuals in images, influenced by their distance from the camera and the image resolution.
    \item {\bf Occlusions:} Individuals may be partially obscured by objects or other people in the environment.
    \item {\bf Crowded Scenes:} Pose estimation becomes challenging in highly crowded locations, where occlusions and the presence of many individuals hinder accurate detection.
\stopitemize

A significant obstacle in developing models to address these challenges lies in the training data. Popular datasets like MPII(TODO: REFERENCE), AI Challenger(TODO: REFERENCE), and COCO (\scc Tsung-Yi, 2015) mainly feature unoccluded individuals close to the camera in non-crowded scenes. Although specialized datasets like CrowdPose, OCHuman, and Tiny People Pose have been introduced to tackle specific concerns, they each focus on a single issue and present challenges in their annotation styles and validation methods, making it difficult to train a comprehensive model. No single dataset adequately addresses all three main challenges of real-world human pose estimation (\scc Alinezhad Noghre et al., 2022).

\obrazek{coco-annotations}{Keypoint annotations from COCO dataset. Source: (\\scc Alinezhad Noghre et al., 2022)}{figures/coco-annotations.png}{width=\makeupwidth}

In \in{Figure}[coco-annotations], it is evident that individuals who are distant from the camera or in crowded scenes are not annotated. In the upper left image, people riding elephants are not labeled, and in the bottom right image, most of the crowd is also unlabeled. Similarly, individuals distant from the camera in the other images are not annotated, even though they are visible. Hand annotating all these unmarked individuals would be challenging and time-consuming, which explains their absence. The COCO dataset's annotation files contain null keypoint annotations corresponding to individuals who may be present in the image but are not annotated. During validation, if additional skeletons lacking annotations are identified, the count of null key points is deducted. Moreover, COCO automatically disregards all but the 20 skeletons with the highest confidence to prevent undue penalization of networks for estimating skeletons of unlabeled individuals.

These limitations disproportionately affect bottom-up approaches, favored for real-world applications due to their lower computational complexities and better real-time execution capabilities. Unlike top-down approaches, bottom-up methods aim to detect individuals independently. However, the lack of labels in real-world scenarios affects both training and validation, hindering the detection of distant individuals and potentially leading to false positives without proper penalization.

In human pose estimation, there are two primary approaches: {\bf bottom-up} and {\bf top-down}.

\startitemize[n]
    \item {\bf Bottom-up approach:} In this approach, the algorithm first detects individual body parts, or keypoints, such as joints like elbows, knees, and wrists, in the image. These keypoints are then grouped together to form complete human poses. Bottom-up methods are often preferred for their efficiency and scalability, particularly in crowded scenes with multiple individuals.
    \item {\bf Top-down approach:} Conversely, the top-down approach involves first detecting the entire human body in the image and then estimating the positions of individual keypoints. This method usually involves using a person detector to locate individuals and then applying a separate model to estimate their poses. While top-down approaches may offer better accuracy for isolated individuals, they can be computationally more intensive, especially in crowded environments.
\stopitemize

These two approaches each have their advantages and limitations, and the choice between them depends on factors such as the complexity of the scene, computational resources available, and the desired balance between accuracy and efficiency.

% ------- Section ------- %
\pkap[section:neural-network]{Neural Network}
We will now temporarily set aside the challenges inherent in human pose estimation and delve into the mechanics employed by existing detection models. This exploration will afford us a deeper understanding of the underlying processes driving detection methodologies.

\NN\-s, inspired by the structure and function of the {\em human brain}, are computational models comprising {\em interconnected} layers of artificial {\em neurons} responsible for processing and transforming information. Demonstrating remarkable capabilities, \NN\-s have proven effective in diverse tasks, including image recognition, natural language processing, and machine translation. A~schematic representation of a simple \NN\ is presented in \in{Figure}[nn-schema], illustrating individual layers of neurons interconnected with their neighbours. The initial layer is commonly referred to as the {\em input layer}, followed by {\em hidden layers}, and concluding with the {\em output layer}. In practical usage, data, such as an image in the form of a vector where values represent individual pixels, is input into the initial layer for analysis. The \NN\ processes this information, ultimately yielding a result in the form of a single value or vector, dependent on the nature of the problemâ€”be it a classification or regression task. Across various fields, \NN\-s have consistently demonstrated their robustness, excelling in tasks such as classification, prediction, filtering, optimization, pattern recognition, and function approximation (\scc Simoneau et al., 1998).

\obrazek{nn-schema}{Example neural network schema. A~very simple structure introduces the input layer with 6 dimensions followed by the 2 hidden layers. The first has 4 dimensions and the second with 3 dimensions. Finally, the output layer has only 1 dimension. This means, that the multidimensional input given to the \\NN\\ is generalised and expressed just by one number. This is the key concept for classification models. Source: (\\scc Nielsen, 2015)}{figures/neural-network-schema.png}{width=\makeupwidth}

\ppkap[subsection:nn-works]{How Neural Network Works}

A~\NN\, inspired by the human brain, is a computational system organized into layers of artificial neurons (\scc Nielsen, 2015). Each connection between neurons has a {\em weight}, representing the strength of influence (\scc Goodfellow et al., 2016). The network learns by adjusting these weights during training, where it processes input data through layers, utilizes {\em activation functions} to determine neuron \uv{firing}, and iteratively adjusts weights based on the difference between predicted and actual outcomes (\scc Nielsen, 2015; \scc Goodfellow et al., 2016; \scc Mazur, 2015). The forward pass involves making predictions, while the backward pass compares predictions to actual results, adjusting weights to minimize {\em errors} (\scc Mazur, 2015). This learning process enables the neural network to recognize patterns and make accurate decisions in tasks like {\em image recognition} or {\em language processing} (\scc Goodfellow et al., 2016).

% ------- Section ------- %
\pkap[section:cnn]{Convolutional Neural Network}
\CNN\-s are a type of \NN\ architecture that excels at processing and analyzing visual data, such as images and videos. They are particularly well-suited for skeleton detection due to their ability to {\em extract} local features from the input data. \CNN\-s typically consist of a series of {\em convolutional layers}, each of which applies a {\em filter} or {\em kernel} to the input data to extract {\em features}. The filters are learned during the training process, allowing the \CNN\ to learn the patterns and relationships that are important for skeleton detection (\scc Singh, 2019). For a better understanding of the \CNN\ architecture see example \in{Figure}[cnn-schema].

\CNN\-s have several advantages for skeleton detection (\scc Ce et al., 2020):

\startitemize[1]
    \item {\bf Translation Invariance:} \CNN\-s are invariant to small translations in the input data. This is important for skeleton detection, as the human body can be in {\em different positions} in an image or video.
    \item {\bf Feature Learning:} \CNN\-s can learn {\em complex features} from the input data, which is essential for accurate skeleton detection.
    \item {\bf Parameter Sharing:} \CNN\-s share {\em weights} across different positions in the input data. This reduces the number of parameters in the network, making it more efficient and easier to train.
\stopitemize

\CNN\-s have become the dominant architecture for skeleton detection, and they have significantly improved the accuracy of this task (\scc Singh, 2019\; \scc Ce et al., 2020).
\obrazek{cnn-schema}{A simple classification architecture by CNN. Source: (\\scc Koushik, 2023)}{figures/cnn-schema.png}{width=\makeupwidth}

\ppkap[subsection:cnn-works]{How Convolutional Layers Work}

Each convolutional layer in a CNN takes an input image and applies a filter to it to extract features. The filter is a small matrix of weights that slides across the input image, producing a feature map at each position. The feature map is a representation of the input image that highlights the patterns that are relevant to the task at hand (\scc Agarwal et al., 2019).

For example, in the case of human skeleton detection, a filter might be used to extract features that are indicative of human joints, such as the elbows, knees, and wrists. The feature map produced by this filter would highlight the locations of these joints in the input image.

\ppkap[subsection:cnn-pooling-layers]{Pooling Layers}

After the convolutional layers extract features, pooling layers are often used to reduce the dimensionality of the feature maps. This helps to reduce the computational cost of the network and also helps to make the network more invariant to small changes in the input data.

Pooling layers work by dividing the feature map into smaller regions and then taking the maximum or average value of each region. This produces a smaller feature map that still contains the most important features from the original image (\scc Agarwal et al., 2019).

\ppkap[subsection:cnn-fully-connected-layers]{Fully Connected Layers}

Once the feature maps have been extracted and pooled, they are passed through a series of fully connected layers. These layers are similar to the artificial neurons that are found in traditional neural networks. They take an input vector and produce an output vector.

In the case of human skeleton detection, the fully connected layers are used to classify the detected features as either human joints or backgrounds. The output vector from the final fully connected layer is a probability distribution over the possible classes (\scc Agarwal et al., 2019).

\ppkap[subsection:cnn-training]{Training the CNN}

The CNN is trained using a process called {\em supervised learning} (\scc Liu, 2012). This involves providing the network with a dataset of labelled images, where each image is labelled with the positions of the human joints. The network then learns to associate the features extracted from the images with the corresponding labels.

The training process involves adjusting the weights of the filters and connections in the network. This is done using an algorithm called backpropagation (\scc Mazur, 2015), which iteratively updates the weights to minimize the error between the network's predictions and the ground truth labels (\scc Agarwal et al., 2019).

\ppkap[subsection:cnn-example-usage]{Example of CNN Usage}

To illustrate how a CNN is used for human skeleton detection, consider a scenario where a CNN is tasked with detecting human skeletons in a video stream. The CNN would first extract features from each frame of the video using its convolutional layers. Then, it would use these features to predict the positions of the human joints in the frame. This prediction can be used for various analyses of the human body movements in the video.

\ppkap[subsection:cnn-limitations]{Limitations of Current Methods}

While CNNs have achieved significant success in human skeleton detection, there are still some limitations to these methods. One limitation is that CNNs can be {\em computationally expensive}, especially when dealing with {\em high-resolution} images or videos. Additionally, CNNs can be sensitive to {\em noise} and {\em occlusions}, which can make it difficult to accurately detect skeletons in real-world scenarios.

Researchers are continuing to develop new methods to improve the accuracy and efficiency of CNNs for human skeleton detection. These methods include using deeper networks, exploring new architectures, and developing more efficient training algorithms (\scc Agarwal et al., 2019).

% ------- Section ------- %
\pkap[section:rcnn]{Region-based Convolutional Neural Network}
\RCNN\-s are a class of deep \CNN\-s that have been widely used for object detection and localization. They are typically characterized by a {\em two-stage} pipeline that involves {\em region proposal} and {\em region classification} (\scc Ren et al., 2015). In the \in{Figure}[rcnn-stages] is displayed possible detection scenario of the \RCNN\.
\obrazek{rcnn-stages}{RCNN stages. Source: (\\scc Girshick, 2016)}{figures/rcnn-stages.png}{width=\makeupwidth}

\startitemize[1]
    \item {\bf Region Proposal:} The first stage of an \RCNN\ involves generating a set of region proposals, which are candidate {\em bounding boxes} for objects in the input image. These proposals are typically generated using a {\em selective search algorithm} (\scc He et al., 2015) that identifies regions that are likely to contain objects based on their visual saliency and spatial context (\scc Girshick et al., 2016).
    \item {\bf Feature Extraction and Classification:} The second stage of an \RCNN\ involves classifying each region proposal as either {\em containing} the object or {\em not} (\scc Ren et al., 2015). This is accomplished by using a \CNN\ to extract feature vectors from each proposal and then applying a classifier to determine whether the features are indicative of the object (\scc Girshick et al., 2016).
\stopitemize

The original \RCNN\ architecture has been criticized for its computational {\em inefficiency}, as it involves two separate stages of processing (\scc Ren et al., 2015). To address this issue, researchers developed {\em Faster R-CNN}, which integrates the region proposal and region classification stages into a {\em single network} (\scc Ren et al., 2015). This significantly reduces the computational cost and improves the overall performance of the system (\scc He et al., 2015).
% ------- Section ------- %
% \pkap[section:transformation-models]{Transformation Models of \NN\-s}
% Transformation models aim to improve the performance and efficiency of \NN\-s by transforming the input or output data. These models can be used to reduce the dimensionality of the data, improve the interpretability of the model, or adapt the model to specific tasks.

% ------- Section ------- %
\pkap[section:existing-nns]{Existing \NN\-s for Human Pose Estimation}
Several \NN\ architectures have been developed for skeleton detection. This thesis explores three notable examples, each with a dedicated section in this chapter:

\startitemize[n]
    \item {\bf PoseNet:} Lightweight and efficient \CNN\ for real-time single-person detection.
    \item {\bf MoveNet:} Family of lightweight models for real-time human pose estimation on mobile devices. Used the {\em lightning} version for single-person detection.
    \item {\bf MMPose:} Library uses a \CNN\ for multiple human pose estimation.
\stopitemize


% ------- Section ------- %
\pkap[section:posenet]{PoseNet}
{\em Pose_landmark} (\PoseNet\) is a single-person detection model from the MediaPipe family that is used to detect keypoints or pose landmarks on the human body in images and videos. It is a \CNN\--based model that uses a {\em two-stage} pipeline to first detect person {\em bounding box} and then refine the detection by {\em estimating} the positions of {\bf 33} {\em keypoints} on detected person (\scc Posenet, 2024). The output structure of the {\em PoseNet} model can be found in \in{Figure}[posenet-skeleton].

The first stage of the pipeline, the person detection stage, uses a Single Shot MultiBox Detector (\SSD\) to generate a bounding box around the person in the input image. The SSD is a lightweight and efficient \CNN\ architecture that is well-suited for real-time applications (\scc PoseNet, 2024).

The second stage of the pipeline, the pose estimation stage, uses a \CNN\ to refine the person detections by estimating the positions of 33 keypoints on the detected person. The keypoints are typically located on the joints of the human body, such as the elbows, knees, and wrists (\scc PoseNet, 2024).

The \PoseNet\ model is trained on a large COCO dataset with images and videos of people performing a variety of actions. This training data helps the model to learn to identify the keypoints on human bodies in a variety of poses and orientations. In the Table below can be found some of the key features of the \PoseNet\ model.

\TABULKA[][tab:posenet-features]{PoseNet model features}
    \setupTABLE[r][1][style=bold]
    \setupTABLE[c][each][offset=3dd]
    \setupTABLE[frame=off]
    \setupTABLE[r][1][topframe=on,bottomframe=on]
    \setupTABLE[c][each][leftframe=on]
    \setupTABLE[c][1][leftframe=off]
    \bTR
        \bTD Feature\eTD\bTD    Description\eTD\eTR
    \bTR
        \bTD Input\eTD\bTD      RGB image or video frame\eTD\eTR
    \bTR
            \bTD Output\eTD\bTD     Pose landmarks for a person detected in the input\eTD\eTR
    \bTR
        \bTD Landmarks\eTD\bTD  33 keypoints\eTD\eTR
    \bTR
        \bTD Accuracy\eTD\bTD   Up to 83\% accuracy on the COCO dataset\eTD\eTR
    \bTR
        \bTD Speed\eTD\bTD      10 - 20 FPS\eTD\eTR

\obrazek{posenet-skeleton}{PoseNet skeleton structure with IDs to each keypoint. The skeleton representation plays a crucial role in introducing the unified format as described in \\in{section}[section:unified-format] on \\at{page}[section:unified-format]. Source: (\\scc PoseNet, 2024).}{figures/posenet-detection-structure.png}{width=34cc}

% ------- Section ------- %
\pkap[section:movenet]{MoveNet}
MoveNet is a family of {\em lightweight} and {\em efficient} pose estimation models developed by Google AI for {\em real-time} human pose estimation. In this thesis, the {\em lightning} version of the model was used. It is designed for mobile and embedded devices. MoveNet employs a {\em two-stage} pipeline to achieve real-time performance while maintaining high {\em accuracy} (\scc MoveNet, 2024). The output structure of the {\em MoveNet} model can be found in \in{Figure}[movenet-skeleton].

The first stage is responsible for detecting and predicting the rough location of the human body in an image or video frame. It utilizes a \SSD\ architecture to generate {\em bounding box} around the potential person (\scc MoveNet, 2024).

The second stage refines the pose estimation results by utilizing a single-person pose estimation model. This model takes the one bounding box predicted in the first stage and refines it to pinpoint the locations of {\bf 17} {\em keypoints} on the one detected person. The keypoints correspond to prominent joints in the human body, such as the elbows, knees, hips, and shoulders (\scc Khanh, 2021).

The single-person pose estimation model utilizes a heatmap-based approach, where each keypoint is associated with a heatmap that indicates the probability of the keypoint being present at a particular location in the image. The model then refines the bounding box by iteratively adjusting it to maximize the overall likelihood of the keypoints being within the bounding box (\scc Khanh, 2021).

MoveNet focus on detecting the pose of the person who is closest to the image centre and ignores the other people who are in the image frame (i.e. background people rejection) (\scc Google, 2021).

The pose refinement process is repeated multiple times to improve the accuracy of the pose estimation results. The final output is a set of 17 keypoints for the one detected person. These keypoints provide a detailed representation of the person's pose, including the positions of their joints, limbs, and other landmarks (\scc Khanh, 2021).

\TABULKA[][tab:movenet-features]{MoveNet model features}
    \setupTABLE[r][1][style=bold]
    \setupTABLE[c][each][offset=3dd]
    \setupTABLE[frame=off]
    \setupTABLE[r][1][topframe=on,bottomframe=on]
    \setupTABLE[c][each][leftframe=on]
    \setupTABLE[c][1][leftframe=off]
    \bTR
        \bTD Feature\eTD\bTD    Description\eTD\eTR
    \bTR
        \bTD Input\eTD\bTD      RGB image or video frame\eTD\eTR
    \bTR
        \bTD Output\eTD\bTD     Pose landmarks for a person detected in the input\eTD\eTR
    \bTR
        \bTD Landmarks\eTD\bTD  17 keypoints\eTD\eTR
    \bTR
        \bTD Accuracy\eTD\bTD   Up to 88\% on the COCO dataset\eTD\eTR
    \bTR
        \bTD Speed\eTD\bTD      Up to 30 FPS\eTD\eTR

\obrazek{movenet-skeleton}{MoveNet skeleton structure with IDs to each keypoint. This model simplifies the pose detection process compared to the PoseNet described in \\in{section}[posenet-skeleton] on \\at{page}[posenet-skeleton], which contributes to its superior performance. As a result, the MoveNet detection results do not contribute significantly to the accuracy of the unified format described in \\in{section}[section:unified-format] on \\at{page}[section:unified-format].}{figures/movenet-detection-structure.png}{width=34cc}

% ------- Section ------- %
\pkap[section:mmpose]{MMPose}
This section describes the model and architecture used for multiple human pose estimation in the {\em MMPose} library (\scc MMPose, 2020). The model is based on a \CNN\ that is trained on a large dataset of images and their corresponding ground truth human poses. The network can predict the positions of {\bf 133} {\em keypoints} on the human body. In addition to {\bf 17} {\em body} keypoints, model detects {\bf 68} {\em face} keypoints, {\bf 21} {\em lefthand} keypoints, {\bf 21} {\em righthand} keypoints, {\bf 6} {\em feet} keypoints. The output structure of the {\em MMPose} model can be found in \in{Figure}[mmpose-skeleton].

The model is divided into {\em two} main stages. The first stage detects human bodies in the input image. This is done using a {\em Faster R-CNN} detector, which is a {\em two-stage} object detection network. The detector first extracts a set of {\em region proposals} from the image, and then {\em classifies} each proposal as either a {\em human} or {\em not} (\scc Ke et al., 2019).

The second stage estimates the poses of the detected human bodies. This is done using a {\em top-down} pose estimation network, which is a \CNN\ that takes as input the bounding boxes of the detected bodies and outputs a set of heatmaps that represent the probability of each keypoint being located at each pixel in the image (\scc Ke et al., 2019).

The top-down pose estimation network is based on the {\em HRNet} architecture, which is a deep \CNN\ that is designed for human pose estimation. The network consists of a series of {\em residual blocks}, each of which consists of two convolutional layers with a {\em stride} of 1 followed by two convolutional layers with a stride of 2. This allows the network to capture both local and global information in the image (\scc Ke et al., 2019).

The human pose estimation results are then evaluated using the COCO WholeBody metric (\scc Jin et al., 2020; \scc Xe et al., 2022), which is a measure of the accuracy of the predicted keypoints.

\TABULKA[][tab:mmpose-features]{MMPose model features}
    \setupTABLE[r][1][style=bold]
    \setupTABLE[c][each][offset=3dd]
    \setupTABLE[frame=off]
    \setupTABLE[r][1][topframe=on,bottomframe=on]
    \setupTABLE[c][each][leftframe=on]
    \setupTABLE[c][1][leftframe=off]
    \bTR
        \bTD Feature\eTD\bTD    Description\eTD\eTR
    \bTR
        \bTD Input\eTD\bTD      RGB image or video frame\eTD\eTR
    \bTR
        \bTD Output\eTD\bTD     List of pose landmarks for each person detected in the input\eTD\eTR
    \bTR
        \bTD Landmarks\eTD\bTD  133 keypoints\eTD\eTR
    \bTR
        \bTD Accuracy\eTD\bTD   76.3\% on the COCO WholeBody dataset\eTD\eTR
    \bTR
        \bTD Speed\eTD\bTD      Requires a powerful GPU for real-time use\eTD\eTR

\obrazek{mmpose-skeleton}{MMPose skeleton structure with IDs of used keypoint in the further processing. For simplicity, the small blue points do not have ID ensuring good visibility. Additionally, the blue keypoints have been omitted to achieve the unified format described in \\in{section}[section:unified-format] on \\at{page}[section:unified-format].}{figures/mmpose-detection-structure.png}{width=36cc,height=41cc}

% % ------- Section ------- %
\pkap[section:chapter-metrics]{Metrics}
In this section, we take a closer look at how to effectively measure the model accuracy concerning the pose estimation. Multiple metrics, such as Average Percentage Error (\APE\), Mean Squared Error (\MSE\) and others will be investigated for the usability for pose estimation evaluation.

% ------- Sub-section ------- %
\ppkap[subsection:ape]{Average Percentage Error}
The \APE\ is a metric commonly used to evaluate the accuracy of human pose detection models. It measures the average difference between the predicted keypoint locations and their corresponding ground truth locations in a pose annotation(for each human instance separately, then averages all instances).

The \APE\ is calculated for each pose prediction in a dataset. Here's the breakdown:

\startitemize[n]
    \item {\bf Distance Calculation:} The {\bf Euclidean} distance between each predicted keypoint and its corresponding ground truth keypoint is calculated.
    \item {\bf Averaging:} The individual distances are then averaged across all keypoints for a single pose.
    \item {\bf Normalization:} To account for image size variations, the average distance is normalized by the maximum dimension (width or height) of the image containing the pose. This normalization is achieved by dividing the average distance by the maximum dimension obtained from the bounding box information in the ground truth data.
    \item {\bf Percentage Conversion:} Finally, the normalized average distance is multiplied by 100 to express the error as a percentage.
\stopitemize

The Euclidean distance between the predictions and the annotated dataset is computed as follows:

\startplaceformula[reference=euclidean-distance]
    \startformula {D}_2(x,\tilde{x}) = \fraction{1}{N} \sum_{i=1}^{N} \|\,p_i(x) - d_i(\tilde{x})\,\|^{2}
    \stopformula
\stopplaceformula

where $ p_i (x) $ is the 2D points annotated in the reference image of the database, $ d_i (\tilde{x}) \in  \mathbf R^2 $ is the prediction in the image of the 2D coordinates of the target $ i $ knowing the predicted pose $ \tilde{x} $ (Ababsa et al., 2020).

A~lower APE value indicates a more accurate pose prediction. Ideally, the APE should be as close to 0\% as possible. However, the acceptable APE threshold depends on the specific application and the level of precision required.

Here is the list of advantages:
\startitemize[1]
    \item {\bf Simple to understand:} \APE\ provides a clear and interpretable measure of error.
    \item {\bf Image size agnostic:} Normalization by image size allows for fair comparison across images of varying resolutions.
\stopitemize

And here are some limitations of the \APE\ metric:
\startitemize[1]
    \item {\bf Limited information:} \APE\ only considers the average distance between keypoints, neglecting potential outliers or specific joint errors.
    \item {\bf Normalization dependence:} The accuracy of normalization depends on the quality of bounding box information.
\stopitemize

APE is a valuable metric for evaluating human pose detection models. However, it is recommended to use \APE\ in conjunction with other evaluation metrics, such as {\bf Precision-Recall} (PR) curves or {\bf Object Keypoint Similarity} (OKS), to obtain a more comprehensive understanding of the model performance.

% ------- Sub-section ------- %
\ppkap[subsection:mse]{Mean Squared Error}
The \MSE\ is a metric commonly used to evaluate the accuracy of human pose detection models and is very similar to the \APE\ metric from previous \in{Subsection}[subsection:ape]. It involves squaring the difference ({\bf Euclidean distance}, see \in{Formula}[euclidean-distance]) between each predicted keypoint coordinate and its corresponding ground truth value, summing these squared errors for all keypoints in a pose, and then averaging the sum.

The advantages of this metric is that it focuses on larger errors. By squaring the errors, \MSE\ gives more weight to significant deviations between predicted and ground truth keypoints. This can be helpful in identifying poses with substantial errors in specific joints.

There are also some disadvantage behaviour in this metric, such as sensitivity to outliers. Since squaring amplifies larger errors, \MSE\ can be overly influenced by a single incorrectly predicted keypoint, potentially inflating the overall error score.

Another fact is the interpretability for human. Unlike \APE\, which is a percentage, \MSE\ produces raw squared distance values that are not directly interpretable in terms of accuracy.

\pppkap[subsubsection:ape-mse]{Combining APE and MSE}

Using both \APE\ and \MSE\ provides a more comprehensive view of the model's performance. \APE\ offers a general sense of average error, while \MSE\ highlights poses with substantial keypoint localization issues.

One should consider the normalization for the \MSE\. While normalization by instance size is not strictly necessary for \MSE\, it can be helpful for comparing results across datasets with varying image scales. You can normalize \MSE\ by dividing it by the square of the maximum instance dimension (width or height of the BBOX).
One have to keep in mind the interpretation. When reporting \MSE\, it is crucial to mention that the values represent squared distances and {\bf not percentages} for proper context.

\MSE\ can be a valuable addition to \APE\ for human pose detection evaluation. By combining them, you gain insights into both the average error and the presence of significant localization errors. However, the limitations of MSE needs to be kept in mind to ensure clear interpretation when reporting the results.

% % ------- Section ------- %
\pkap[section:chapter-summary]{Chapter Summary}
This chapter introduced the key concepts of \NN\-s, \CNN\-s, \RCNN\-s and existing models for human pose estimation. \CNN\-s excel in processing visual data for skeleton detection, while \RCNN\-s, including Faster \RCNN\, enhance efficiency through a two-stage pipeline for object detection, alongside existing \NN\ models like PoseNet, MoveNet, and MMPose, achieving high accuracy in real-time human pose estimation. Finally, the key concepts of the detection performance evaluation were explored namely \APE\ and the \MSE\ metrics.
